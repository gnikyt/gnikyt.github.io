   #[1]gnikyt feed

   [2]gnikyt Code ramblings.

                   Handling Shopify API limits and Goroutines

   /* Apr 10, 2025 — 12KB */

   I frequently build integration applications with Shopify, and I thought
   it would be helpful to highlight one approach (of many) for handling
   rate limits with Shopify’s GraphQL API using Go that I often use
   (depending on the project). In this case, the integration was
   originally handled by a third-party provider. However, after they were
   acquired, they shut down their service and the integration needed to be
   rebuilt from scratch.

   The original integration accepted a standardized CSV file from a
   third-party service. This file contained a list of SKUs and their
   inventory levels, which needed to be synced with the Shopify store. The
   same CSV format was used by the service for multiple merchants and
   typically contained between 3,000–3,500 rows. Not every SKU in the CSV
   necessarily existed in the store, so that had to be accounted for
   during processing.

   The new application had to handle the following steps:
    1. Read each line from the CSV
    2. Check if the SKU exists in the store
    3. If it does, update its inventory to the specified value

   Format of the CSV:
ITEM_CODE,QTY
00288,5
22991,1
23211,18
...

   Shopify’s GraphQL API limits would obviously be high priority to factor
   in. In the case for this specific Shopify store, the limits were:
     * 2000 available points
     * 100 points refilled per second

   Each row in the CSV required one to two GraphQL calls to Shopify,
   depending on whether the SKU existed in the store. A query was needed
   to fetch the SKU, and if it was found, a mutation followed which would
   update the inventory. By analyzing the query and mutation costs in
   Shopify’s GraphQL app, it was determined that each query consumed 2
   points, while each mutation consumed 10 points. This meant, in the
   worst case senario, where every SKU existed, a total of 12 points would
   be consumed per row.

   My goal was to maximize the number of concurrent queries and mutations
   without blowing through the available points too quickly. At the same
   time, maintain a safeguard to throttle requests if the point threshold
   was reached, allowing time for points to refill and ensuring the
   updates could continue without having a bad request returned by
   Shopify.

   With the point consumption determined, I developed a script to simulate
   depleating the available points by 12, multiplied by the potential
   concurrent running updates, while also increasing the available points
   at a rate of 100 points per second. This simulation also counted the
   number of requests per second. The result of this testing with a
   different number of potential concurrent running jobs, I came to the
   conclusion that between 15-20 concurrent jobs would be a safe balance;
   potientially draining 180 points per second or more and refilling at a
   rate of 100 per second, would result in an average net decrease of 80
   points per second from the available points. Given a safeguard would be
   in place to handle potentially hitting the threshold of available
   points, this was a great balance to continue with.

   Now, there are several methods a developer can take to craft a solution
   preventing draining the available points down to zero… some may simply
   call each job in sequence or a batch of jobs in sequence and sleep for
   a set time afterwards, some may develop a worker pool system with a
   small set capacity, etc.

   I decided to develop a semaphore approach. If you’re not familiar, a
   semaphore is essentially a concurrency control method to maintain a set
   capacity of “how many” of something is permitted to run at a time. A
   process would “aquire” a spot and when completed it’s work, it would
   “release” the spot, so another process can aquire it.

   Since there was only between 3,000-3,500 rows in the CSV on average, I
   decided to skip a worker pool setup and simply spin up each row of the
   CSV as a Goroutine, where each Goroutine would attempt to aquire a spot
   with the semaphore control and upon release of that spot, we would
   check the remaining available points and handle accordingly.

   Additionally, the capacity of this semaphore would be set to the 15-20
   limit previously determined from the simulated script. If the remaining
   available points dipped below a set threshold, the release mechanism
   would cause a “pause” in releasing, calculating the time it would take
   to refill the available points back to maximum, then resuming the
   release. This would allow 15-20 concurrent jobs to be running at a
   time, while the release mechanism acted as the safe guard to ensure the
   available points were not totally drained.

   Example code is below.

// regulator/point.go
package regulator

import (
    "sync/atomic"
    "time"
)

// Point represents the information of point values and keeps
// track of the remaining points, threshold, limit, and refill rate.
type Point struct {
    Remaining  atomic.Int32 // Points remaining.
    Threshold  int32        // Point value to which we would begin sleeping.
    Limit      int32        // Upper limit of points available.
    RefillRate int32        // Rate of refill of number of points per second.
}

// Update accepts a new value of remaining points to store.
func (pts *Point) Update(points int32) {
    pts.Remaining.Store(points)
}

// RefillDuration accounts for the remaining points, the limit, and the refill r
ate to
// determine how many seconds it would take to refill to remaining points back t
o full.
// It will return a duration which can be used to sleep.
func (pts *Point) RefillDuration() time.Duration {
    return time.Duration((tp.Limit-tp.Remaining.Load())/tp.RefillRate) * time.Se
cond
}

// AtThreshold will return true or false if we have reached or surpassed the set
// threshold of remaining points or not.
func (pts *Point) AtThreshold() bool {
    return tp.Remaining.Load() <= tp.Threshold
}

// regulator/regulator.go
package regulator

import (
    "sync"
    "time"
)

var AquireBuffer = 200 * time.Millisecond // Buffer of time to wait before attem
pting to re-aquire a spot.

// Regulator is responsible regulating when to pause and resume processing of Go
routines.
// Points remaining, point thresholds, and point refill rates are taken into
// consideration. If remaining points go below the threshold, a pause is initiat
ed
// which will also calculate how long a pause should happen based on the refill
rate.
// Once pause is completed, the processing will resume. A PauceFunc and ResumeFu
nc
// can optionally be passed in which will fire respectively when a pause happens
// and when a resume happens.
type Regulator struct {
    *Point // Point information and tracking.

    PauseFunc  func(int32, time.Duration) // Optional callback for when pause ha
ppens.
    ResumeFunc func()                     // Optional callback for when resume h
appens.

    pausedAt time.Time     // When paused last happened.
    sema     chan struct{} // Semaphore for controlling the number of Goroutines
 running.

    mu     sync.Mutex // For handling paused flag control.
    paused bool       // Pause flag.
}

// NewRegulator returns a pointer to a Regulator. It accepts a cap which represe
nts the
// capacity of how many Goroutines can run at a time, it also accepts informatio
n
// about the point parameters and lastly, optional paramters.
func New(cap int, point *Point, opts ...func(*Regulator)) *Regulator {
    reg := &Regulator{
        Point: point,
        sema:  make(chan struct{}, cap),
    }
    for _, opt := range opts {
        opt(reg)
    }
    if reg.PauseFunc == nil {
        // Provide default PauseFunc.
        withPauseFunc(func(_ int32, _ time.Duration) {})(reg)
    }
    if reg.ResumeFunc == nil {
        // Provide default ResumeFunc.
        withResumeFunc(func() {})(reg)
    }
    // Set the remaining points to the limit of points.
    reg.Update(point.Limit)
    return reg
}

// Aquire will attempt to aquire a spot to run the Goroutine.
// It will continue in a loop until it does aquire also pausing
// if the pause flag has been enabled. Aquiring is throttled at
// the value for AquireBuffer.
func (reg *Regulator) Aquire() {
    var aquired bool
    for !aquired {
        // Factor in pause flag. Looping will cause a "pause".
        for {
            if !reg.paused {
                break
            }
        }

        // Attempt to aquire a spot, if not we will throttle the next loop.
        select {
        case reg.sema <- struct{}{}:
            aquired = true
        default:
            time.Sleep(AquireBuffer)
        }
    }
}

// Release will release a spot for another Goroutine to take.
// It accepts a current value of remaining points, to which the
// remaining points will only be updated if the count is greater than -1.
// If the remaining points is below the set threshold, a pause will be
// initiated and a duration of this pause will be calculated based
// upon several factors surrouding the point information such as limit,
// threshold, and the refull rate.
func (reg *Regulator) Release(points int32) {
    defer reg.mu.Unlock()
    reg.mu.Lock()

    reg.Update(points)
    if reg.AtThreshold() {
        // Calculate the duration required to refill and that duration time has
passed
        // before we call for a pause.
        ra := reg.RefillDuration() + PauseBuffer
        if reg.pausedAt.Add(ra).Before(time.Now()) {
            reg.paused = true
            reg.pausedAt = time.Now()
            reg.PauseFunc(points, ra)

            go func() {
                time.Sleep(ra)
                reg.paused = false
                reg.ResumeFunc()
            }()
        }
    }

    <-reg.sema
}

// withPauseFunc is a functional option for the Regulator to
// call when a pause happens. The points remaining and the
// duration of the pause will passed into the function.
func withPauseFunc(fn func(int32, time.Duration)) func(*Regulator) {
    return func(reg *Regulator) {
        reg.PauseFunc = fn
    }
}

// withResumeunc is a functional option for the Regulator to
// call when resume from a pause happens.
func withResumeFunc(fn func()) func(*Regulator) {
    return func(reg *Regulator) {
        reg.ResumeFunc = fn
    }
}

package processor

const (
    Retries    int           = 3               // Number of times to retry a fai
led row processing.
    RetryDelay time.Duration = 1 * time.Second // Delay for between the retries.

    Capacity int = 15 // Number of Goroutines to be able to run at once.

    PointThreshold  int32 = 200  // The threshold of when we should pause.
    PointLimit      int32 = 2000 // Maximum number of points available.
    PointRefillRate int32 = 100  // Refill rate of points per second.
)

//
// ...
//

p.Regulator := regulator.New(
    Capacity,
    regulator.Point{
        Threshold: PointThreshold,
        Limit: PointLimit,
        RefillRate: PointRefillRate,
    },
)

//
// ...
//

func (proc *processor) runJob(row []string) {
    proc.regulator.Aquire()
    points, err := retry(proc.processJob(row))
    proc.postProcessJob(row, err)
    proc.regulator.Release(points)
}

//
// ...
//

func (proc *processor) Run() {
    proc.timeStart = time.Now()

    read, closer, err := proc.newReader()
    defer closer()
    if err != nil {
        log.Printf("[error] run: read: %s\n", err)
        proc.ctxCancel()
        return
    }

    // Skip the header row.
    _, err = read()
    if err != nil {
        log.Printf("[error] run: row: %s\n", err)
        proc.ctxCancel()
        return
    }

    // Read rest of rows outside of the header.
    for loop := true; loop; {
        row, err := read()
        if errors.Is(err, io.EOF) {
            break
        }
        if err != nil {
            proc.ctxCancel()
            log.Fatalf("[error] run: row: %s\n", err)
        }

        select {
        case <-proc.ctx.Done():
            // Context closed, stop processing rows.
            log.Println("[info] run: ctx exited due to abort or timeout")
            loop = false
        default:
            // Run the job.
            proc.jwg.Add(1)
            go proc.runJob(row)
        }
    }

    go func() {
        proc.jwg.Wait()
        close(proc.done)
    }()
    <-proc.done

    proc.timeEnd = time.Now()
    if err = proc.SendSummary(); err != nil {
        log.Printf("[error] run: %s\n", err)
    }
}

//
// ...
//

   Using our above semaphore method, we are allowing 15 Goroutines to run
   concurrently out of the 3,000-3,500 Goroutines, where each upon each
   Goroutine’s completion, the Goroutine will report the remaining points
   back to the release mechanism, which will determine if a pause is
   needed before actually issuing the release.

   The result was a success for this project… the inventory updates we’re
   able to complete between 3 1/2 to 4 1/2 minutes without hitting the
   threshold often. Hopefully this helpful to those looking to do similar.

   [3]MD | [4]TXT | [5]CC-4.0
     __________________________________________________________________

   [6]Ty King

Ty King

   A self-taught, seasoned, and versatile developer from Newfoundland.
   Crafting innovative solutions with care and expertise. See more
   [7]about me.
   [8]Github [9]LinkedIn [10]CV [11]RSS
     *
     *
     *
     *
     *
     *
     *
     *
     *
     *

References

   Visible links:
   1. /rss.xml
   2. /
   3. /handling-shopify-api-limits-goroutines/index.md
   4. /handling-shopify-api-limits-goroutines/index.txt
   5. https://creativecommons.org/licenses/by/4.0/
   6. /about
   7. /about
   8. https://github.com/gnikyt
   9. https://linkedin.com/in/gnikyt
  10. /assets/files/cv.pdf
  11. /rss.xml

   Hidden links:
  13. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-1
  14. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-2
  15. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-3
  16. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-4
  17. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-5
  18. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-6
  19. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-7
  20. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-8
  21. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-9
  22. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-10
  23. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-11
  24. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-12
  25. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-13
  26. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-14
  27. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-15
  28. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-16
  29. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-17
  30. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-18
  31. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-19
  32. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-20
  33. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-21
  34. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-22
  35. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-23
  36. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-24
  37. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-25
  38. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-26
  39. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-27
  40. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-28
  41. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-29
  42. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-30
  43. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-31
  44. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-32
  45. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-33
  46. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb2-34
  47. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-1
  48. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-2
  49. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-3
  50. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-4
  51. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-5
  52. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-6
  53. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-7
  54. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-8
  55. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-9
  56. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-10
  57. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-11
  58. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-12
  59. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-13
  60. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-14
  61. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-15
  62. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-16
  63. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-17
  64. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-18
  65. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-19
  66. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-20
  67. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-21
  68. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-22
  69. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-23
  70. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-24
  71. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-25
  72. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-26
  73. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-27
  74. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-28
  75. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-29
  76. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-30
  77. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-31
  78. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-32
  79. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-33
  80. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-34
  81. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-35
  82. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-36
  83. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-37
  84. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-38
  85. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-39
  86. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-40
  87. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-41
  88. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-42
  89. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-43
  90. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-44
  91. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-45
  92. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-46
  93. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-47
  94. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-48
  95. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-49
  96. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-50
  97. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-51
  98. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-52
  99. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-53
 100. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-54
 101. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-55
 102. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-56
 103. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-57
 104. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-58
 105. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-59
 106. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-60
 107. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-61
 108. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-62
 109. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-63
 110. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-64
 111. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-65
 112. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-66
 113. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-67
 114. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-68
 115. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-69
 116. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-70
 117. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-71
 118. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-72
 119. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-73
 120. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-74
 121. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-75
 122. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-76
 123. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-77
 124. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-78
 125. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-79
 126. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-80
 127. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-81
 128. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-82
 129. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-83
 130. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-84
 131. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-85
 132. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-86
 133. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-87
 134. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-88
 135. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-89
 136. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-90
 137. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-91
 138. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-92
 139. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-93
 140. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-94
 141. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-95
 142. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-96
 143. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-97
 144. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-98
 145. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-99
 146. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-100
 147. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-101
 148. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-102
 149. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-103
 150. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-104
 151. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-105
 152. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-106
 153. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-107
 154. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-108
 155. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-109
 156. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-110
 157. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-111
 158. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-112
 159. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-113
 160. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-114
 161. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-115
 162. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-116
 163. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-117
 164. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-118
 165. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-119
 166. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-120
 167. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-121
 168. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-122
 169. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-123
 170. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-124
 171. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-125
 172. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb3-126
 173. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-1
 174. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-2
 175. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-3
 176. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-4
 177. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-5
 178. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-6
 179. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-7
 180. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-8
 181. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-9
 182. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-10
 183. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-11
 184. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-12
 185. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-13
 186. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-14
 187. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-15
 188. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-16
 189. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-17
 190. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-18
 191. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-19
 192. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-20
 193. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-21
 194. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-22
 195. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-23
 196. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-24
 197. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-25
 198. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-26
 199. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-27
 200. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-28
 201. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-29
 202. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-30
 203. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-31
 204. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-32
 205. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-33
 206. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-34
 207. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-35
 208. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-36
 209. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-37
 210. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-38
 211. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-39
 212. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-40
 213. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-41
 214. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-42
 215. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-43
 216. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-44
 217. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-45
 218. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-46
 219. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-47
 220. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-48
 221. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-49
 222. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-50
 223. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-51
 224. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-52
 225. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-53
 226. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-54
 227. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-55
 228. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-56
 229. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-57
 230. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-58
 231. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-59
 232. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-60
 233. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-61
 234. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-62
 235. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-63
 236. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-64
 237. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-65
 238. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-66
 239. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-67
 240. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-68
 241. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-69
 242. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-70
 243. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-71
 244. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-72
 245. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-73
 246. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-74
 247. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-75
 248. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-76
 249. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-77
 250. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-78
 251. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-79
 252. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-80
 253. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-81
 254. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-82
 255. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-83
 256. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-84
 257. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-85
 258. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-86
 259. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-87
 260. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-88
 261. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-89
 262. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-90
 263. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-91
 264. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-92
 265. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-93
 266. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-94
 267. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-95
 268. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-96
 269. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-97
 270. localhost/tmp/lynxXXXXOmu7Kk/L1367018-7683TMP.html#cb4-98
